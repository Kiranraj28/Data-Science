{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7bdaade",
   "metadata": {},
   "source": [
    "# FIFA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704aa45a",
   "metadata": {},
   "source": [
    "- Task 1:-Prepare a complete data analysis report on the given data.\n",
    "\n",
    "- Task 2:- Explore football skills and cluster football players based on their attributes.\n",
    "\n",
    "- Task3:- Explore the data and attempt all the below asked questions in a\n",
    "  step by step manner:\n",
    "  \n",
    "    - Prepare a rank ordered list of top 10 countries with most players. Which\n",
    "      countries are producing the most footballers that play at this level?\n",
    "      \n",
    "    - Plot the distribution of overall rating vs. age of players. Interpret what is the\n",
    "      age after which a player stops improving?\n",
    "      \n",
    "    - Which type of offensive players tends to get paid the most: the striker, the\n",
    "      right-winger, or the left-winger?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27767bfb",
   "metadata": {},
   "source": [
    "# Task 1:-Prepare a complete data analysis report on the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b672fe8",
   "metadata": {},
   "source": [
    "# BASIC LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c04596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sweetviz as sv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc960a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv('PRCP-1004-Fifa20.zip')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be86cb",
   "metadata": {},
   "source": [
    "# BASIC CHECKS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.info(verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ac554",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.tolist() # here we can see all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700944c",
   "metadata": {},
   "source": [
    "- 61 numerical features and 43 categorical features total 104 features are present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00901295",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() .T   # describe only for numerbical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d46fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = data.describe().columns.tolist() # it shows the all numerical colunms.\n",
    "numerical_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a225785",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col = data.describe(include=object).columns.tolist()\n",
    "categorical_col   # it shows the categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdcd71",
   "metadata": {},
   "source": [
    "# EDA PROCESS :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743603f",
   "metadata": {},
   "source": [
    "- In our dataset , totaly 104 input features are there. \n",
    "- By using Matplotlib & seaborn take a so much of time to see relationship and distribution of data in a dataset. \n",
    "- Upcoming analysis we use the matplotlib and seaborn for essential feature.\n",
    "- Here, For the large no. of input features(104) we use the sweetviz to understanding dataset structure (correlations, distributions, missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = sv.analyze(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.show_html('sweetviz_report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417579f",
   "metadata": {},
   "source": [
    "# TASK 3 (i)\n",
    "\n",
    "Prepare a rank ordered list of top 10 countries with most players. Which\n",
    "countries are producing the most footballers that play at this level?\n",
    "\n",
    "we need the following features from the dataset:\n",
    "Required Features:\n",
    "\n",
    "- Nationality - Identifies the player's country.\n",
    "- Player ID (or Name) - Unique identifier for counting distinct players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= data.groupby('nationality')['short_name'].count() \n",
    "# here we groupby the nationality and make the count player name wise\n",
    "\n",
    "x=x.sort_values(ascending=False) #here sorted the fitlered value in descending order \n",
    "x = x[:10]   # here we extract the top ten countries which produce the most footballers\n",
    "dict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=list(x.keys()),y=list(x.values),label=x,palette=\"cividis\")\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('No.of.playes')\n",
    "plt.xticks(rotation=30)\n",
    "plt.title('Top 10 countries with most players')\n",
    "plt.legend(bbox_to_anchor=(1.3,1),title ='Top 10 countries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04e964",
   "metadata": {},
   "source": [
    "# insights\n",
    "- England produces the highest number of players, significantly ahead of other nations.\n",
    "- Germany and Spain rank second and third, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae8388",
   "metadata": {},
   "source": [
    "# TASK 3 (ii)\n",
    "\n",
    "\n",
    "\n",
    "To analyze the distribution of overall rating vs. age and determine the age when players stop \n",
    "improving, we need:\n",
    "\n",
    "-  Age - The player's age.\n",
    "-  Overall Rating - The overall skill rating of the player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='white')\n",
    "sns.lineplot(x='age',y='overall',data=data,marker='o',markerfacecolor='red')\n",
    "\n",
    "\n",
    "peak_age = data.groupby('age')['overall'].mean().idxmax()\n",
    "# here we find the peakage of overall\n",
    "\n",
    "\n",
    "plt.axvline(peak_age,linestyle='--',label=f'Peak age {peak_age}',color='black')\n",
    "# above code to plot the verticle line in peak age through the x axis.\n",
    "             \n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Overall Rating')\n",
    "plt.legend(bbox_to_anchor=(1,1),fontsize=8)\n",
    "plt.title('Overall vs Age')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afcf6d",
   "metadata": {},
   "source": [
    "# insights\n",
    "- From the above visualization analysis, after the age 41 the player stop improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fee34",
   "metadata": {},
   "source": [
    "# TASK 3 (iii)\n",
    "\n",
    "\n",
    "To determine which type of offensive player (Striker, Right-Winger, or Left-Winger) tends to get \n",
    "paid the most, we need the following columns:\n",
    "\n",
    "- Position - Identifies if the player is a Striker (ST), Right-Winger (RW), or Left-Winger (LW).\n",
    "- Wage (€ or any currency) - The player's salary (how much they get paid).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we filtering the where the Striker, Right-Winger(RW) and Left-Winger(LW) players are played.\n",
    "\n",
    "st_col =[]  # here we store the Striker players wage_eur\n",
    "rw_col =[]  # here we store the Right-Winger(RW) players wage_eur\n",
    "lw_col =[]  # here we store the Left-Winger(LW) players wage_eur\n",
    "\n",
    "x=['ST','RW','LW'] \n",
    "for i in range(len(data)):\n",
    "    for j in x:\n",
    "        if j in str(data.player_positions[i]): \n",
    "            if j=='ST':\n",
    "                st_col.append(data.wage_eur[i])\n",
    "            elif j=='RW':\n",
    "                rw_col.append(data.wage_eur[i])\n",
    "            elif j=='LW':\n",
    "                lw_col.append(data.wage_eur[i])\n",
    "                \n",
    "avg_st_pay = np.mean(st_col) # here we finding the average payment of the Strik position player\n",
    "avg_rw_pay = np.mean(rw_col) # here we finding the average payment of the Right-Winger position player\n",
    "avg_lw_pay = np.mean(lw_col) # here we finding the average payment of the Left-Winger position player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a1ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_data= pd.DataFrame({'Position':['Striker','Right-Winger(RW)','Left-Winger(LW)'],\n",
    "                        'Average_Pay':[avg_st_pay,avg_rw_pay,avg_lw_pay]})\n",
    "# here creating the dataframe for EDA\n",
    "pay_data = pay_data.sort_values(by='Average_Pay',ascending=False) # here we sorting in decending order.\n",
    "pay_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Position',y='Average_Pay',data=pay_data,palette=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05598c1f",
   "metadata": {},
   "source": [
    "# insights\n",
    "- From the Above analysis shows the  offence player Left-Winger(LW) tends to get paid the most comparing to others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff7234",
   "metadata": {},
   "source": [
    "\n",
    "# DATA PREPROCESSING :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaad243",
   "metadata": {},
   "source": [
    "# (i) NULL VALUES HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf538a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.isnull().sum()  # Here we can see the where the null values are present in the dataset\n",
    "y=x[x>0]\n",
    "x[x>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[x>0].count()   #Total no.of colunms has null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f637870",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_null = ((data.isnull().sum())/len(data))*100  \n",
    "# here we can see the null values in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',None)\n",
    "per_null  # it shows the all features how much percentage of null values are present\n",
    "per_null[per_null>0]  # it shows the where are null values present and shows there persentage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fe6b3",
   "metadata": {},
   "source": [
    "### Separate the columns into categorical and numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3da75",
   "metadata": {},
   "source": [
    "## NUMERICAL COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66472ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_num_col =[]   # here we extrac the numerical columns which contains the null values \n",
    "\n",
    "for i in numerical_col:\n",
    "    if i in y:\n",
    "        null_num_col.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56098e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_num_col # the numerical columns which contains the null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we handling the null values in the numerical columns  \n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn = KNNImputer(n_neighbors=3)\n",
    "\n",
    "for i in null_num_col:\n",
    "    \n",
    "    per_null_num =((data[i].isnull().sum())/len(data))*100\n",
    "    \n",
    "    if per_null_num <=20:    \n",
    "        data[i]=data[i].fillna(np.mean(data[i]))\n",
    "        \n",
    "    elif per_null_num <=80:\n",
    "        data[[i]] = knn.fit_transform(data[[i]])\n",
    "        \n",
    "    else:\n",
    "        data.drop(columns=[i],inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60d73c",
   "metadata": {},
   "source": [
    "#### A column has <= 20 percentage  null values has been replaced by the mean values\n",
    "#### A column has <=80 percentage  null values has been replaced by the kkn values\n",
    "#### A column has >=80 percentage  null values has been droped     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14c0c4",
   "metadata": {},
   "source": [
    "## CATEGORICAL COLUMNS :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cate_col =[]  # here we can see the categorical columns which contains the null values \n",
    "\n",
    "for j in categorical_col:\n",
    "    if j in y:\n",
    "        null_cate_col.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cate_col       # the categorical columns which contains the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in null_cate_col:\n",
    "    \n",
    "    per_null_cate =((data[j].isnull().sum())/len(data))*100\n",
    "    \n",
    "    if per_null_cate<=80:   \n",
    "        data[j] = data[j].fillna(data[j].mode()[0])\n",
    "\n",
    "    else:\n",
    "        data[j] = data[j].fillna(data[j].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da8c20",
   "metadata": {},
   "source": [
    "#### In categorical feature, a feature has less than 80% null values means it replaced by the most frequent value by using of mode\n",
    "#### In categorical feature, a feature has more than 80% null values means it should be drop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba51cf4",
   "metadata": {},
   "source": [
    "# (ii) DUPLICATE VALUES HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum() # In this dataset there is no duplicate (rowise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98428487",
   "metadata": {},
   "source": [
    "#### In our dataset there is no duplicates are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee4fb60",
   "metadata": {},
   "source": [
    "# FEATURE TRANSFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eaa38c",
   "metadata": {},
   "source": [
    "# (iii)  ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_col  # total no.of categroical features are 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ce4a0",
   "metadata": {},
   "source": [
    "## insights\n",
    "- From the above list there are no ordinal features are present  in the categorical_col.\n",
    "- Show, we handle feature in the Nominal way.\n",
    "- Our dataset has 18,000 features, avoid One-Hot Encoding for high-cardinality columns. Instead, we use Frequency Encoding to control feature explosion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77921794",
   "metadata": {},
   "source": [
    "## FREQUENCY ENCODING TECHNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72166e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we checking the no.of unique values present in the each features\n",
    "\n",
    "for i in categorical_col:\n",
    "    uni_len = len(data[i].unique())  \n",
    "    print(i,'-',uni_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73578596",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_col:\n",
    "    uni_len = len(data[i].unique())  \n",
    "    if uni_len <=5000:\n",
    "        fre_data = data[i].value_counts() # Converts counts into probabilities (useful for statistical models).\n",
    "        data[i]=data[i].map(fre_data)\n",
    "    \n",
    "    elif uni_len>5000:\n",
    "        data.drop(columns=[i],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2002b3d1",
   "metadata": {},
   "source": [
    "#### In categorical feature, a feature has less than 5000 unique values means it will handle by Frequency Encoding Technique.\n",
    "#### In categorical feature, a feature has more than 5000 unique values means it should be drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30089019",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info(verbose=False) # here clearly see no categorical features are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fc19d",
   "metadata": {},
   "source": [
    "# To find the correlation of each feature in the dataset.\n",
    "# To take the most correlated feature to bulid the model for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa16e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_data = data.corr()\n",
    "cor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = cor_data.columns.to_list() # collecting the columns\n",
    "index = cor_data.index.to_list()   # collecting the index\n",
    "\n",
    "cor = {}  # here we collecting the correlation value and they correalated variables\n",
    "\n",
    "for i in columns:\n",
    "    for j in index:\n",
    "        if( f'{i},{j}' and f'{j},{i}') not in cor:\n",
    "            if i not in  j:    # Because i and j in string  format so we using string membership\n",
    "                cor[f'{i},{j}']=data[i].corr(data[j])\n",
    "            \n",
    "cor # this variable contains the correlation value of every combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e759782",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cor) # length of correlation variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebf7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # here we separting the positive and negative correlation values\n",
    "x=cor.values() \n",
    "pos_cor = [] # collecting positive values\n",
    "neg_cor = [] # collecting negative values\n",
    "\n",
    "for i in x:\n",
    "    if i>0:\n",
    "        pos_cor.append(i)\n",
    "    elif i<0:\n",
    "        neg_cor.append(i)\n",
    "        \n",
    "top_pos_cor = sorted(pos_cor,reverse=True)[:15] # here we take top 10 positive correlated feature for model buliding \n",
    "top_neg_cor = sorted(neg_cor)[:15] # here we take top 10 negative correlated feature for model buliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neg_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a={} # top 10 Positvie correlated features\n",
    "b={} # top 10 negative correlated features\n",
    "\n",
    "for i in cor.keys():\n",
    "    for j in top_pos_cor:\n",
    "        if cor[i]==j and len(a)<=15:\n",
    "            a[i]=j\n",
    "    for k in top_neg_cor:\n",
    "        if cor[i]==k:\n",
    "            b[i]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a #  top 10 Positvie correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78574bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "b # top 10 Negative correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "features =[]  # here we extracting the unique value form the dictionary keys\n",
    "for i in a.keys():\n",
    "    spl=i.split(',')\n",
    "    if spl[0] not in features:\n",
    "        features.append(spl[0])\n",
    "    if spl[1] not in features:\n",
    "        features.append(spl[1])\n",
    "        \n",
    "for j in b.keys():\n",
    "    spl1=j.split(',')\n",
    "    if spl1[0] not in features:\n",
    "        features.append(spl1[0])\n",
    "    if spl1[1] not in features:\n",
    "        features.append(spl1[1])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "features  # this variable has the both positive and negative high correlative features name for model building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features) # Totally 29 highly correlated features are taken for model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75d9de",
   "metadata": {},
   "source": [
    "# HEATMAP\n",
    "- It shows the correlation matrix of the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874eabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[50,50])\n",
    "sns.heatmap(cor_data,annot=True,annot_kws={'size':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe0c3c",
   "metadata": {},
   "source": [
    "# (iv) SCALING PROCESS\n",
    "- Encoding technique, we use the Frequency encoding. \n",
    "- It assign the Each categories as large numerical value. Because feature has the large data 18k+\n",
    "- Before appling into modeling we need to scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "scaled_data = scaler.fit_transform(data)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scal = pd.DataFrame(scaled_data,columns=data.columns)\n",
    "data_scal # dataframe of scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2557eb",
   "metadata": {},
   "source": [
    "# TASK 2 : To explore football skills and cluster players based on their attributes.\n",
    "\n",
    "we need to select  features  from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd6d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_attributes = features\n",
    "skill_attributes\n",
    "# selected feature based an the correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfbf82",
   "metadata": {},
   "source": [
    "- Above selected features has the high correlation  in the dataset. So, we selected this.\n",
    "- From the above selected feature we sepaterate the numerical and categorical features.\n",
    "- We already separate the overall categorical features (categorical_col) from the whole input features.\n",
    "- From the (categorical_col), here filter needed categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d95f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(skill_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "for i in data_scal.columns:\n",
    "    if i in skill_attributes:\n",
    "        x.append(i)\n",
    "    else:\n",
    "        y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y # for used to remove the features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_fea = data_scal.drop(columns=y)\n",
    "sel_fea  # selected features for model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80675a47",
   "metadata": {},
   "source": [
    "#### Due to large features model will not perform well. \n",
    "#### So we apply the PCA to extract the new features for the model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.90)  # 0.90 indicates that give me a 90% of information covered PCA.\n",
    "pca_data = pca.fit_transform(sel_fea)\n",
    "pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3473bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = pd.DataFrame(pca_data,columns=['PC1','PC2','PC3','PC4','PC5'])\n",
    "pca_data  # extacted principal component dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# List to store inertia values\n",
    "inertia = []\n",
    "\n",
    "# Range of k values to try\n",
    "k_values = range(2, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(pca_data)  \n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot inertia vs. k\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method to Determine Optimal k')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346b46b",
   "metadata": {},
   "source": [
    "## insights\n",
    "- From the above Elbow chart, we can use k value as [3,4,5] anyone.\n",
    "- we use the k=4 to bulid the model or make the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia # here, shows the each k value inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kmeans clustering\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "cluster = kmeans.fit_predict(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "sil_score = silhouette_score(pca_data,cluster)\n",
    "sil_score           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d3a0a",
   "metadata": {},
   "source": [
    "## insights\n",
    "- KMeans silotte score = 0.6360542212538013, which indicates that the Clusters are  well separated.\n",
    "- For increasing the silohtte score , we move on to the KMeans++."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b3cd7",
   "metadata": {},
   "source": [
    "# K-MEANS ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb58b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# List to store inertia values\n",
    "inertia = []\n",
    "\n",
    "# Range of k values to try\n",
    "k_values = range(2, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans_plus = KMeans(n_clusters=k,init='k-means++', random_state=21)\n",
    "    kmeans_plus.fit(pca_data)  \n",
    "    inertia.append(kmeans_plus.inertia_)\n",
    "\n",
    "# Plot inertia vs. k\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method to Determine Optimal k')\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58719d6a",
   "metadata": {},
   "source": [
    "## insights\n",
    "- From the above Elbow chart, we can use k value as [3,4,5] anyone.\n",
    "- we use the k=4 to bulid the model or make the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023260ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_plus = KMeans(n_clusters=4,init='k-means++')\n",
    "k_plus_predict = k_plus.fit_predict(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score_kplus = silhouette_score(pca_data,k_plus_predict)\n",
    "sil_score_kplus      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891794ed",
   "metadata": {},
   "source": [
    "- There is no improvement in the silohtte score\n",
    "- Dataset has any outliers means Kmeans and Kmeans++ are not well perform. \n",
    "- Now we are move on to the advance alogrithm DBSCAN.\n",
    "- DBSCAN  is best algorithm for the outliers detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9005bc8",
   "metadata": {},
   "source": [
    "# **DBSCAN**\n",
    "\n",
    "Before applying **DBSCAN**, we need to determine key parameters such as **ε (Epsilon)** and **MinPts (Minimum Points).**  \n",
    "\n",
    "## **1. Finding the MinPts**  \n",
    "A commonly used heuristic for selecting MinPts is the **\"Rule of Thumb\"**, which states:  \n",
    "\n",
    "### **Formula:**  \n",
    "\\[\n",
    "{MinPts} = 2 * {(Number of Dimensions)}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MinPts = 2*(len(pca_data.columns))\n",
    "MinPts     # Our MinPts is 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c0c44",
   "metadata": {},
   "source": [
    "##  2. Finding the esp:\n",
    "- By the help of MinPts and the NearestNeighbor to calculate best esp value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "near = NearestNeighbors(n_neighbors=MinPts)   # Set up the Nearest Neighbors model\n",
    "near.fit(pca_data)   # Fit the model to the data\n",
    "distance,points = near.kneighbors(pca_data)  # Find distances to the nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08720d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_distance = distance[:,-1]\n",
    "k_distance   # it gives the 4th nearest Neighbor of the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_distance=np.sort(k_distance)\n",
    "k_distance # here we sorted in the ascending order low to high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d16023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the eps value through the graph\n",
    "\n",
    "plt.plot(range(len(k_distance)),k_distance)\n",
    "plt.xlabel(' 4th nearest Points')\n",
    "plt.ylabel('Distance')\n",
    "plt.title(\"K-Distance Plot for DBSCAN eps Selection\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911c233",
   "metadata": {},
   "source": [
    "## insights\n",
    "- From the above Elbow curve observation, we choose the esp =0.1 or 0.12.\n",
    "- Now we know the MinPts=10 and esp=0.12.\n",
    "- Apply those parameter values in the DBSCAN to bulid the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.16,min_samples=MinPts)\n",
    "db_pred = dbscan.fit_predict(pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb36eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score_DBSCAN = silhouette_score(pca_data,db_pred)\n",
    "sil_score_DBSCAN             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236b3e2",
   "metadata": {},
   "source": [
    "#### There is no improvement in the silohtte score by using advance algorithm DBSCAN.\n",
    "#### So, we build the semi-supervied model to prove the my model gives the best prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da81492",
   "metadata": {},
   "source": [
    "# SEMI SUPERVISED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "labels  # Get cluster labels assigned by KMeans for each datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315fc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can classified the no.of datapoints present in the each cluster\n",
    "\n",
    "label_0 =[]  #1924 data points present in the cluster 0\n",
    "label_1 =[] #14317 data points present in the cluster 1\n",
    "label_2 =[] #2037 data points present in the cluster 2\n",
    "\n",
    "for i in labels:\n",
    "    if i==0:\n",
    "        label_0.append(i)\n",
    "    elif i==1:\n",
    "        label_1.append(i)\n",
    "    elif i==2:\n",
    "        label_2.append(i)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_fea = sel_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da8b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "    label_fea['Labels']=labels # here we adding the label columns to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  accuracy_score ,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c39912",
   "metadata": {},
   "source": [
    "# DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8889872",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = label_fea.drop(columns=['Labels'])\n",
    "y = label_fea.Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20,random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e7a640",
   "metadata": {},
   "source": [
    "# MODEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2675587",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080156e4",
   "metadata": {},
   "source": [
    "#### From the model kmeans, take a ouputs to store in  \" labels\" variable and add to the dataset and named as a label_fea.\n",
    "#### By using of Label_fea  to bulid  a semi-supervised model by using Logistic Regression.\n",
    "#### The builded semi-supervised  model gives me a accuracy 0.9991794310722101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16eb15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
